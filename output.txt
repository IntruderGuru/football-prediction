===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\output.txt =====


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\requirements.txt =====
pandas
numpy
scikit-learn
lightgbm
requests
matplotlib
seaborn
plotly
jupyter
pyyaml
tqdm
pyarrow
aiohttp
bs4
understat
fastparquet
joblib
catboost
imbalanced-learn


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\setup_guide.txt =====
# Tworzenie środowiska (z conda)
conda create -n footpred python=3.11 -y
conda activate footpred

# Instalacja zależności
pip install -r requirements.txt

# (Opcjonalnie) zablokowanie wersji
# pip freeze > requirements.lock.txt

# Formatowanie i lintowanie
pip install pre-commit ruff black
pre-commit install

# (Dla PowerShella)
$env:PYTHONPATH="."

# (Dla Linux/macOS)
# export PYTHONPATH=.

# Przetwarzanie danych
python scripts/fetch_football_data_uk.py
python scripts/fetch_understat.py
python scripts/merge_sources.py

# Trenowanie i ewaluacja modelu
python -m src.pipeline --algo lgb

# Predykcja pojedynczego meczu
python scripts/model_predict.py --features "{\"xG_home\":1.4, ...}" --model output/final_model_lgb.pkl


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\output\report.txt =====
              precision    recall  f1-score   support

           A       0.54      0.51      0.52       543
           D       0.33      0.23      0.27       402
           H       0.56      0.69      0.62       691

    accuracy                           0.51      1636
   macro avg       0.47      0.47      0.47      1636
weighted avg       0.50      0.51      0.50      1636


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\scripts\fetch_football_data_uk.py =====
import requests
import pandas as pd
import re
import logging
import itertools
from pathlib import Path
from typing import List, Optional
from src.constants import SEASONS, DIV_MAP, LEAGUES

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

BASE_URL = "https://www.football-data.co.uk/mmz4281/{season}/{code}.csv"
RAW_DIR = Path("data/raw/football_data_uk")
MERGED_OUT = Path("data/raw/football_data_merged.csv")


def fetch_file(season: str, code: str) -> Optional[Path]:
    url = BASE_URL.format(season=season, code=code)
    try:
        response = requests.get(url, timeout=20)
        response.raise_for_status()
        RAW_DIR.mkdir(parents=True, exist_ok=True)
        out_path = RAW_DIR / f"fd_{code}_{season}.csv"
        out_path.write_bytes(response.content)
        logger.info(f"Saved {out_path.name}")
        return out_path
    except Exception as e:
        logger.warning(f"Failed to fetch {url}: {e}")
        return None


def fetch_all() -> List[Path]:
    results = []
    for season, code in itertools.product(SEASONS, LEAGUES):
        path = fetch_file(season, code)
        if path:
            results.append(path)
    return results


def parse_filename(fp: Path) -> Optional[str]:
    m = re.match(r"fd_(?P<div>[A-Z0-9]+)_(?P<season>\d{4}).csv", fp.name)
    if not m:
        logger.warning(f"Regex failed on filename: {fp.name}")
        return None
    div = m["div"].upper()
    if div not in DIV_MAP:
        logger.warning(f"Unknown league code in DIV_MAP: {div} from {fp.name}")
        return None
    return DIV_MAP[div]


def merge_fetched(files: List[Path]) -> pd.DataFrame:
    rows = []
    for fp in files:
        league = parse_filename(fp)
        if league is None:
            continue
        try:
            df = pd.read_csv(
                fp,
                usecols=[
                    "Date",
                    "HomeTeam",
                    "AwayTeam",
                    "FTHG",
                    "FTAG",
                    "B365H",
                    "B365D",
                    "B365A",
                ],
            )
            df = df.rename(
                columns={
                    "Date": "date",
                    "HomeTeam": "home_team",
                    "AwayTeam": "away_team",
                    "FTHG": "home_goals",
                    "FTAG": "away_goals",
                    "B365H": "bookie_home",
                    "B365D": "bookie_draw",
                    "B365A": "bookie_away",
                }
            )
            df["league"] = league
            df["date"] = pd.to_datetime(df["date"], dayfirst=True).dt.date
            rows.append(df)
            logger.info(f"Parsed: {fp.name}")
        except Exception as e:
            logger.warning(f"Error reading {fp.name}: {e}")
    if not rows:
        raise ValueError("No valid CSVs could be parsed.")
    return pd.concat(rows, ignore_index=True)


def main() -> None:
    fetched_files = fetch_all()
    merged_df = merge_fetched(fetched_files)
    MERGED_OUT.parent.mkdir(parents=True, exist_ok=True)
    merged_df.to_csv(MERGED_OUT, index=False)
    logger.info(f"Saved merged CSV to {MERGED_OUT} ({merged_df.shape[0]} rows)")


if __name__ == "__main__":
    main()


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\scripts\fetch_understat.py =====
import asyncio
import aiohttp
import pandas as pd
from understat import Understat
from pathlib import Path
from typing import List, Dict, Any
from src.constants import SEASONS
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

RAW_DIR = Path("data/raw")
RAW_DIR.mkdir(parents=True, exist_ok=True)

UNDERSTAT_LEAGUE_MAP = {
    "EPL": "EPL",
    "LALIGA": "La_liga",
    "BUNDESLIGA": "Bundesliga",
    "SERIEA": "Serie_A",
    "LIGUE1": "Ligue_1",
}

LEAGUES = list(UNDERSTAT_LEAGUE_MAP.keys())


async def get_understat_data(league: str, season: int) -> List[Dict[str, Any]]:
    async with aiohttp.ClientSession() as session:
        understat = Understat(session)
        data = await understat.get_league_results(
            league_name=league, season=str(season)
        )
        return data


def fetch_all_understat(leagues: List[str], seasons: List[str]) -> pd.DataFrame:
    all_dfs = []
    for league in leagues:
        understat_name = UNDERSTAT_LEAGUE_MAP[league]
        for season in seasons:
            year = int("20" + season[:2])
            logger.info(f"Fetching {league} ({understat_name}) season {year}")
            try:
                data = asyncio.run(get_understat_data(understat_name, year))
                df = pd.DataFrame(data)
                df["season"] = season
                df["league"] = league
                all_dfs.append(df)
            except Exception as e:
                logger.warning(f"Failed to fetch {league} {season}: {e}")
    if not all_dfs:
        raise ValueError("No data fetched.")
    return pd.concat(all_dfs, ignore_index=True)


def main() -> None:
    df = fetch_all_understat(LEAGUES, SEASONS)
    df.to_csv(RAW_DIR / "understat_all.csv", index=False)
    logger.info(f"Saved Understat data to understat_all.csv ({df.shape[0]} rows)")


if __name__ == "__main__":
    main()


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\scripts\merge_sources.py =====
import pandas as pd
import ast
import numpy as np
from pathlib import Path
import pyarrow as pa
import pyarrow.parquet as pq
from src.constants import TEAM_MAP


def merge_sources():

    f = pd.read_csv("data/raw/football_data_merged.csv", parse_dates=["date"])

    f["date"] = pd.to_datetime(f["date"], errors="coerce")
    f["date_only"] = f["date"].dt.normalize()

    u = pd.read_csv("data/raw/understat_all.csv")
    u["datetime"] = pd.to_datetime(u["datetime"], errors="coerce")
    u = u[u["datetime"].notna()]
    u["date"] = u["datetime"].dt.normalize()

    def parse_json(x, key, cast_type):
        try:
            return cast_type(ast.literal_eval(x)[key])
        except (ValueError, SyntaxError, KeyError, TypeError):
            return np.nan

    u["home_goals_us"] = u["goals"].apply(lambda s: parse_json(s, "h", int))
    u["away_goals_us"] = u["goals"].apply(lambda s: parse_json(s, "a", int))
    u["xG_home"] = u["xG"].apply(lambda s: parse_json(s, "h", float))
    u["xG_away"] = u["xG"].apply(lambda s: parse_json(s, "a", float))

    u["home_team_normed"] = u["h"].apply(
        lambda x: TEAM_MAP.get(
            ast.literal_eval(x)["title"].strip(), ast.literal_eval(x)["title"].strip()
        )
    )
    u["away_team_normed"] = u["a"].apply(
        lambda x: TEAM_MAP.get(
            ast.literal_eval(x)["title"].strip(), ast.literal_eval(x)["title"].strip()
        )
    )

    f["league_normed"] = f["league"].astype(str).str.strip().str.upper()
    u["league_normed"] = u["league"].astype(str).str.strip().str.upper()

    u_sub = u[
        [
            "date",
            "league_normed",
            "home_team_normed",
            "away_team_normed",
            "xG_home",
            "xG_away",
        ]
    ].copy()

    f_sub = f[
        [
            "date",
            "league",
            "league_normed",
            "home_team",
            "away_team",
            "home_goals",
            "away_goals",
            "bookie_home",
            "bookie_draw",
            "bookie_away",
        ]
    ].copy()

    merged = f_sub.merge(
        u_sub,
        left_on=["date", "league_normed", "home_team", "away_team"],
        right_on=["date", "league_normed", "home_team_normed", "away_team_normed"],
        how="left",
    )

    merged["season"] = merged["date"].dt.year

    avg_xg_home = (
        merged[merged["xG_home"].notna()]
        .groupby(["league_normed", "season"])["xG_home"]
        .mean()
        .reset_index()
        .rename(columns={"xG_home": "avg_xg_home"})
    )

    avg_xg_away = (
        merged[merged["xG_away"].notna()]
        .groupby(["league_normed", "season"])["xG_away"]
        .mean()
        .reset_index()
        .rename(columns={"xG_away": "avg_xg_away"})
    )

    merged = merged.merge(avg_xg_home, on=["league_normed", "season"], how="left")
    merged = merged.merge(avg_xg_away, on=["league_normed", "season"], how="left")

    mask_home = merged["xG_home"].isna()
    merged.loc[mask_home, "xG_home"] = merged.loc[mask_home, "avg_xg_home"]

    mask_away = merged["xG_away"].isna()
    merged.loc[mask_away, "xG_away"] = merged.loc[mask_away, "avg_xg_away"]

    merged["bookie_sum"] = (
        1.0 / merged["bookie_home"]
        + 1.0 / merged["bookie_draw"]
        + 1.0 / merged["bookie_away"]
    )
    merged["bookie_prob_home"] = (1.0 / merged["bookie_home"]) / merged["bookie_sum"]
    merged["bookie_prob_draw"] = (1.0 / merged["bookie_draw"]) / merged["bookie_sum"]
    merged["bookie_prob_away"] = (1.0 / merged["bookie_away"]) / merged["bookie_sum"]

    merged["result"] = np.select(
        [
            merged["home_goals"] > merged["away_goals"],
            merged["home_goals"] < merged["away_goals"],
        ],
        ["H", "A"],
        default="D",
    )

    merged = merged[
        [
            "date",
            "league",
            "home_team",
            "away_team",
            "home_goals",
            "away_goals",
            "bookie_home",
            "bookie_draw",
            "bookie_away",
            "xG_home",
            "xG_away",
            "bookie_sum",
            "bookie_prob_home",
            "bookie_prob_draw",
            "bookie_prob_away",
            "result",
        ]
    ].copy()

    Path("data/processed").mkdir(parents=True, exist_ok=True)

    table_all = pa.Table.from_pandas(merged, preserve_index=False)
    pq.write_table(table_all, "data/processed/merged.parquet")

    model_input = merged.dropna(
        subset=["xG_home", "xG_away", "bookie_home", "bookie_draw", "bookie_away"]
    )

    schema = pa.schema(
        [
            ("date", pa.timestamp("ns")),
            ("home_team", pa.string()),
            ("away_team", pa.string()),
            ("home_goals", pa.int64()),
            ("away_goals", pa.int64()),
            ("bookie_home", pa.float64()),
            ("bookie_draw", pa.float64()),
            ("bookie_away", pa.float64()),
            ("league", pa.string()),
            ("xG_home", pa.float64()),
            ("xG_away", pa.float64()),
            ("bookie_sum", pa.float64()),
            ("bookie_prob_home", pa.float64()),
            ("bookie_prob_draw", pa.float64()),
            ("bookie_prob_away", pa.float64()),
            ("result", pa.string()),
        ]
    )

    table_model = pa.Table.from_pandas(model_input, schema=schema, preserve_index=False)
    pq.write_table(table_model, "data/processed/model_input.parquet")

    with open("data/processed/model_input.txt", "w", encoding="utf-8") as fout:
        fout.write(model_input.to_string(index=False))


if __name__ == "__main__":
    merge_sources()


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\scripts\model_predict.py =====
import argparse
import json
import joblib
import ast
from pathlib import Path

import numpy as np
import pandas as pd


def load_schema(schema_path: str) -> list[str]:
    path = Path(schema_path)
    if not path.exists():
        raise FileNotFoundError(f"❌ Brak pliku schematu kolumn: {path}")
    return json.loads(path.read_text())


def build_feature_df(raw_json: str | Path, schema: list[str]) -> pd.DataFrame:

    try:
        if isinstance(raw_json, Path):

            txt = raw_json.read_text(encoding="utf-8-sig")
        else:
            txt = raw_json
        txt = txt.strip()
        if (txt.startswith("'") and txt.endswith("'")) or (
            txt.startswith('"') and txt.endswith('"')
        ):
            txt = txt[1:-1]
        try:
            feats = json.loads(txt)
        except json.JSONDecodeError as e:
            feats = ast.literal_eval(txt)
    except Exception as e:
        raise ValueError(f"❌ Błąd podczas parsowania cech: {e}")

    missing = [c for c in schema if c not in feats]
    if missing:
        raise ValueError(f"Brakuje feature’ów: {missing}")

    return pd.DataFrame([[feats[c] for c in schema]], columns=schema)


def main() -> None:
    ap = argparse.ArgumentParser(description="Inferencja - pojedynczy mecz")
    ap.add_argument("--model", default="output/final_model_lgb.pkl")
    ap.add_argument("--schema", default="output/feature_schema.json")

    g = ap.add_mutually_exclusive_group(required=True)
    g.add_argument(
        "--features",
        help=(
            "inline JSON z cechami, np. "
            '\'{"bookie_prob_home":0.45,"away_roll_xg_5":0.1, ...}\''
        ),
    )

    g.add_argument("--features-file", help="plik JSON z feature’ami")

    args = ap.parse_args()

    schema = load_schema(args.schema)
    model = joblib.load(args.model)

    X = (
        build_feature_df(Path(args.features_file), schema)
        if args.features_file
        else build_feature_df(args.features, schema)
    )

    proba = model.predict_proba(X)[0]
    pred = model.classes_[np.argmax(proba)]

    print(f"Prediction  : {pred}")
    print(f"Probabilities: {dict(zip(model.classes_, proba.round(3)))}")


if __name__ == "__main__":
    main()


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\scripts\model_train.py =====
import json
import argparse
import joblib
import os
from src.data_loader import load_data
from src.features import extract_features
from src.model import train_model
from src.constants import FEATURE_COLUMNS


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--algo", default="rf", choices=["rf", "lgb"])
    parser.add_argument(
        "--params", default=None, help="path to JSON with tuned hyper-params"
    )
    args = parser.parse_args()

    df = extract_features(load_data()).dropna(subset=FEATURE_COLUMNS + ["result"])
    X, y = df[FEATURE_COLUMNS], df["result"]

    params = {}
    if args.params:
        with open(args.params) as f:
            params = json.load(f)

    model = train_model(X, y, algo=args.algo, params=params)

    os.makedirs("models", exist_ok=True)
    joblib.dump(model, f"models/final_model_{args.algo}.pkl")
    print(f"Model saved to models/final_model_{args.algo}.pkl")


if __name__ == "__main__":
    main()


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\scripts\predict_match.py =====
import argparse, json, joblib
from datetime import datetime
import numpy as np
import pandas as pd
from pathlib import Path
from scipy.stats import poisson

from src.features import extract_features
from src.constants import FEATURE_COLUMNS

MODEL_PATH = Path("output/final_model_lgb.pkl")
SCHEMA_PATH = Path("output/feature_schema.json")
DATA_PATH = Path("data/processed/model_input.parquet")


def poisson_probs(lam_hf, lam_ha, lam_af, lam_aa, max_goals=8):

    p_home = p_draw = p_away = 0.0
    for hg in range(max_goals + 1):
        for ag in range(max_goals + 1):
            p = poisson.pmf(hg, lam_hf) * poisson.pmf(ag, lam_af)
            if hg > ag:
                p_home += p
            elif hg < ag:
                p_away += p
            else:
                p_draw += p
    s = p_home + p_draw + p_away
    return p_home / s, p_draw / s, p_away / s


def build_feature_row(home, away, date, odds):

    df_hist = pd.read_parquet(DATA_PATH)
    df_hist = df_hist[df_hist["date"] < date]

    fake = {
        "date": date,
        "home_team": home,
        "away_team": away,
        "home_goals": np.nan,
        "away_goals": np.nan,
        "xG_home": np.nan,
        "xG_away": np.nan,
        "bookie_prob_home": np.nan,
        "bookie_prob_draw": np.nan,
        "bookie_prob_away": np.nan,
    }
    df = pd.concat([df_hist, pd.DataFrame([fake])], ignore_index=True)
    features = extract_features(df).iloc[-1]

    if odds is None:
        p_home, p_draw, p_away = poisson_probs(
            features["lambda_home_for"],
            features["lambda_home_against"],
            features["lambda_away_for"],
            features["lambda_away_against"],
        )
    else:
        p_home, p_draw, p_away = odds

    features[["bookie_prob_home", "bookie_prob_draw", "bookie_prob_away"]] = [
        p_home,
        p_draw,
        p_away,
    ]
    row = features[FEATURE_COLUMNS].astype("float64")
    return pd.DataFrame([row], columns=FEATURE_COLUMNS)


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--home", required=True)
    ap.add_argument("--away", required=True)
    ap.add_argument("--date", required=True, help="YYYY-MM-DD")
    ap.add_argument("--odds-home", type=float)
    ap.add_argument("--odds-draw", type=float)
    ap.add_argument("--odds-away", type=float)
    args = ap.parse_args()

    date_obj = pd.to_datetime(args.date)
    odds = None
    if args.odds_home and args.odds_draw and args.odds_away:
        inv = [1 / x for x in (args.odds_home, args.odds_draw, args.odds_away)]
        s = sum(inv)
        odds = [x / s for x in inv]

    X = build_feature_row(args.home, args.away, date_obj, odds)
    model = joblib.load(MODEL_PATH)

    proba = model.predict_proba(X)[0]
    pred = model.classes_[np.argmax(proba)]

    print(f"{args.home} - {args.away}  ({args.date})")
    print(
        f"P(H)={proba[model.classes_=='H'][0]:.2%} · "
        f"P(D)={proba[model.classes_=='D'][0]:.2%} · "
        f"P(A)={proba[model.classes_=='A'][0]:.2%}"
    )
    print(f"Prediction: {pred}")


if __name__ == "__main__":
    main()


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\scripts\tune_lgbm.py =====
import json
from pathlib import Path

from src.data_loader import load_data
from src.features import extract_features
from src.model import lgb_grid_search
from src.constants import FEATURE_COLUMNS


if __name__ == "__main__":

    df = extract_features(load_data()).dropna()
    X = df[FEATURE_COLUMNS]
    y = df["result"]

    best_params, best_score = lgb_grid_search(X, y)
    print("Best CV score:", best_score)
    Path("output").mkdir(exist_ok=True)
    json.dump(best_params, open("output/lgb_best.json", "w"), indent=4)
    print("Saved → output/lgb_best.json")


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\src\constants.py =====
DIV_MAP = {
    "E0": "EPL",
    "SP1": "LALIGA",
    "D1": "BUNDESLIGA",
    "I1": "SERIEA",
    "F1": "LIGUE1",
}

SEASONS = ["2425", "2324", "2223", "2122", "2021", "1920"]
LEAGUES = list(DIV_MAP.keys())

TEAM_MAP = {
    # ——— Premier League / Championship
    "Manchester United": "Man United",
    "Manchester City": "Man City",
    "Wolverhampton Wanderers": "Wolves",
    "Brighton & Hove Albion": "Brighton",
    "Tottenham Hotspur": "Tottenham",
    "West Ham United": "West Ham",
    "Newcastle United": "Newcastle",
    "Nottingham Forest": "Nott'm Forest",
    "Sheffield United": "Sheffield United",
    "Crystal Palace": "Crystal Palace",
    "Liverpool": "Liverpool",
    "Chelsea": "Chelsea",
    "Arsenal": "Arsenal",
    "Everton": "Everton",
    "Bournemouth": "Bournemouth",
    "Burnley": "Burnley",
    "Southampton": "Southampton",
    "Leicester": "Leicester",
    "Leeds": "Leeds",
    "Aston Villa": "Aston Villa",
    "Watford": "Watford",
    "Wolves": "Wolves",
    "West Bromwich Albion": "West Brom",
    "Fulham": "Fulham",
    # ——— La Liga
    "Athletic Club": "Ath Bilbao",
    "Atletico Madrid": "Ath Madrid",
    "Real Madrid": "Real Madrid",
    "FC Barcelona": "Barcelona",
    "Real Sociedad": "Sociedad",
    "Real Betis": "Betis",
    "Real Valladolid": "Valladolid",
    "Espanyol": "Espanol",
    "Granada": "Granada",
    "Celta Vigo": "Celta",
    "Sevilla": "Sevilla",
    "Valencia": "Valencia",
    "Villarreal": "Villarreal",
    "Getafe": "Getafe",
    "Levante": "Levante",
    "Alaves": "Alaves",
    "Mallorca": "Mallorca",
    "Elche": "Elche",
    "Cadiz": "Cadiz",
    "Eibar": "Eibar",
    "Osasuna": "Osasuna",
    # ——— Ligue 1
    "Paris Saint Germain": "Paris SG",
    "Olympique Marseille": "Marseille",
    "Lyon": "Lyon",
    "Lille": "Lille",
    "AS Monaco": "Monaco",
    "Nice": "Nice",
    "Stade Rennais": "Rennes",
    "Nantes": "Nantes",
    "Strasbourg": "Strasbourg",
    "Brest": "Brest",
    "Ajaccio": "Ajaccio",
    "Nimes": "Nimes",
    "Montpellier": "Montpellier",
    "Lorient": "Lorient",
    "Bordeaux": "Bordeaux",
    "Reims": "Reims",
    "Amiens": "Amiens",
    "Angers": "Angers",
    "Dijon": "Dijon",
    "Clermont Foot": "Clermont",
    "Metz": "Metz",
    "Lens": "Lens",
    # ——— Serie A
    "Inter Milan": "Inter",
    "AC Milan": "Milan",
    "Juventus": "Juventus",
    "Atalanta": "Atalanta",
    "Napoli": "Napoli",
    "Roma": "Roma",
    "SS Lazio": "Lazio",
    "Fiorentina": "Fiorentina",
    "Torino": "Torino",
    "Udinese": "Udinese",
    "Bologna": "Bologna",
    "Empoli": "Empoli",
    "Sassuolo": "Sassuolo",
    "Cagliari": "Cagliari",
    "Sampdoria": "Sampdoria",
    "Verona": "Verona",
    "Genoa": "Genoa",
    "Salernitana": "Salernitana",
    "Brescia": "Brescia",
    "Lecce": "Lecce",
    "Monza": "Monza",
    # ——— Bundesliga
    "Bayern Munich": "Bayern Munich",
    "Borussia Dortmund": "Dortmund",
    "Bayer Leverkusen": "Leverkusen",
    "RB Leipzig": "RB Leipzig",
    "Borussia M.Gladbach": "M'gladbach",
    "VfB Stuttgart": "Stuttgart",
    "Eintracht Frankfurt": "Ein Frankfurt",
    "FC Cologne": "FC Koln",
    "Union Berlin": "Union Berlin",
    "Freiburg": "Freiburg",
    "Hertha Berlin": "Hertha",
    "Hoffenheim": "Hoffenheim",
    "Wolfsburg": "Wolfsburg",
    "Mainz 05": "Mainz",
    "Augsburg": "Augsburg",
    "Schalke 04": "Schalke 04",
    "Werder Bremen": "Werder Bremen",
    "Fortuna Duesseldorf": "Fortuna Dusseldorf",
    "Greuther Fuerth": "Greuther Furth",
    "FC Heidenheim": "Heidenheim",
    "Arminia Bielefeld": "Bielefeld",
    # ——— Inne
    "Holstein Kiel": "Holstein Kiel",
    "Ipswich": "Ipswich",
    "Como": "Como",
    "Parma Calcio 1913": "Parma",
    "RasenBallsport Leipzig": "RB Leipzig",
    "Rayo Vallecano": "Vallecano",
    "Saint-Etienne": "St Etienne",
    "St. Pauli": "St. Pauli",
}

FEATURE_COLUMNS: list[str] = [
    "bookie_prob_home",
    "bookie_prob_draw",
    "bookie_prob_away",
    "home_roll_xg_5",
    "away_roll_xg_5",
    "home_roll_gd_5",
    "away_roll_gd_5",
    "home_roll_form_5",
    "away_roll_form_5",
    "dow",
    "month",
    "home_days_since",
    "away_days_since",
    "elo_home",
    "elo_away",
    "lambda_home_for",
    "lambda_home_against",
    "lambda_away_for",
    "lambda_away_against",
]

WEIGHTS_RF = {"H": 1, "A": 1, "D": 1.1}
WEIGHTS_LGB = {"H": 1, "A": 1, "D": 1.2}


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\src\data_loader.py =====
import pandas as pd


def load_data(path: str = "data/processed/model_input.parquet") -> pd.DataFrame:
    
    return pd.read_parquet(path)

===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\src\features.py =====
import pandas as pd

K_ELO = 20
WINDOW = 5


def extract_features(df: pd.DataFrame) -> pd.DataFrame:
    

    df = df.sort_values("date").copy()


    df["xg_diff"] = df["xG_home"] - df["xG_away"]
    df["goal_diff"] = df["home_goals"] - df["away_goals"]
    df["home_pts"] = df["result"].map({"H": 3, "D": 1, "A": 0})
    df["away_pts"] = df["result"].map({"H": 0, "D": 1, "A": 3})


    df["home_roll_xg_5"] = df.groupby("home_team")["xg_diff"].transform(
        lambda s: s.shift().rolling(WINDOW).mean()
    )
    df["away_roll_xg_5"] = df.groupby("away_team")["xg_diff"].transform(
        lambda s: s.shift().rolling(WINDOW).mean()
    )
    df["home_roll_gd_5"] = df.groupby("home_team")["goal_diff"].transform(
        lambda s: s.shift().rolling(WINDOW).mean()
    )
    df["away_roll_gd_5"] = df.groupby("away_team")["goal_diff"].transform(
        lambda s: s.shift().rolling(WINDOW).mean()
    )
    df["home_roll_form_5"] = df.groupby("home_team")["home_pts"].transform(
        lambda s: s.shift().rolling(WINDOW).sum()
    )
    df["away_roll_form_5"] = df.groupby("away_team")["away_pts"].transform(
        lambda s: s.shift().rolling(WINDOW).sum()
    )


    df["dow"] = df["date"].dt.weekday
    df["month"] = df["date"].dt.month


    df["home_prev_date"] = df.groupby("home_team")["date"].shift(1)
    df["away_prev_date"] = df.groupby("away_team")["date"].shift(1)
    df["home_days_since"] = (df["date"] - df["home_prev_date"]).dt.days
    df["away_days_since"] = (df["date"] - df["away_prev_date"]).dt.days


    elo_dict: dict[str, float] = {}
    elo_home, elo_away = [], []
    for _, row in df.iterrows():
        h, a = row["home_team"], row["away_team"]
        elo_dict.setdefault(h, 1500.0)
        elo_dict.setdefault(a, 1500.0)
        elo_home.append(elo_dict[h])
        elo_away.append(elo_dict[a])
        score_home = (
            1.0 if row["result"] == "H" else 0.5 if row["result"] == "D" else 0.0
        )
        exp_home = 1 / (1 + 10 ** ((elo_dict[a] - elo_dict[h]) / 400))
        elo_dict[h] += K_ELO * (score_home - exp_home)
        elo_dict[a] += K_ELO * ((1 - score_home) - (1 - exp_home))
    df["elo_home"], df["elo_away"] = elo_home, elo_away


    grp_home = df.groupby("home_team")
    df["lambda_home_for"] = grp_home["home_goals"].transform(
        lambda s: s.shift().expanding().mean()
    )
    df["lambda_home_against"] = grp_home["away_goals"].transform(
        lambda s: s.shift().expanding().mean()
    )

    grp_away = df.groupby("away_team")
    df["lambda_away_for"] = grp_away["away_goals"].transform(
        lambda s: s.shift().expanding().mean()
    )
    df["lambda_away_against"] = grp_away["home_goals"].transform(
        lambda s: s.shift().expanding().mean()
    )


    df.drop(columns=["home_prev_date", "away_prev_date"], inplace=True)

    return df

===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\src\metrics.py =====
import os
import json
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)
import matplotlib.pyplot as plt


def save_classification_report_txt(y_true, y_pred, path="output/report.txt"):
    report = classification_report(y_true, y_pred)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w") as f:
        f.write(report)


def save_classification_report_json(y_true, y_pred, path="output/metrics.json"):
    report_dict = classification_report(y_true, y_pred, output_dict=True)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w") as f:
        json.dump(report_dict, f, indent=4)


def save_confusion_matrix_plot(y_true, y_pred, path="output/confusion_matrix.png"):
    cm = confusion_matrix(y_true, y_pred, labels=["H", "D", "A"])
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["H", "D", "A"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.tight_layout()
    os.makedirs(os.path.dirname(path), exist_ok=True)
    plt.savefig(path)
    plt.close()


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\src\model.py =====
from typing import Literal, Any, Dict, Tuple

from src.constants import WEIGHTS_LGB, WEIGHTS_RF
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, f1_score
from sklearn.model_selection import GridSearchCV

Algo = Literal["rf", "lgb", "cat"]


def _build_rf(params: Dict[str, Any] | None = None) -> RandomForestClassifier:
    cfg = dict(
        n_estimators=300,
        max_depth=10,
        min_samples_leaf=5,
        class_weight=WEIGHTS_RF,
        random_state=42,
        n_jobs=-1,
    )
    if params:
        cfg.update(params)
    return RandomForestClassifier(**cfg)


def _build_lgb(params: Dict[str, Any] | None = None) -> lgb.LGBMClassifier:
    cfg = dict(
        objective="multiclass",
        num_class=3,
        n_estimators=600,
        learning_rate=0.03,
        max_depth=-1,
        subsample=0.8,
        colsample_bytree=0.8,
        class_weight=WEIGHTS_LGB,
        random_state=42,
        n_jobs=-1,
    )
    if params:
        cfg.update(params)
    return lgb.LGBMClassifier(**cfg)


def _build_cat(params: Dict[str, Any] | None = None) -> CatBoostClassifier:
    cfg = dict(
        loss_function="MultiClass",
        iterations=800,
        learning_rate=0.03,
        depth=8,
        auto_class_weights="Balanced",
        random_seed=42,
        verbose=False,
    )
    if params:
        cfg.update(params)
    return CatBoostClassifier(**cfg)


def get_model(algo: Algo = "rf", params: Dict[str, Any] | None = None):
    if algo == "rf":
        return _build_rf(params)
    if algo == "lgb":
        return _build_lgb(params)
    if algo == "cat":
        return _build_cat(params)
    raise ValueError(f"Unknown algo: {algo}")


def train_model(X, y, *, algo: Algo = "rf", params=None):
    model = get_model(algo, params)
    model.fit(X, y)
    return model


def evaluate_model(y_true, y_pred, verbose: bool = True) -> Dict[str, float]:
    if verbose:
        print(classification_report(y_true, y_pred))
    return {
        "macro_f1": f1_score(y_true, y_pred, average="macro"),
        "accuracy": (y_true == y_pred).mean(),
    }


lgb_param_grid = {
    "num_leaves": [31, 63],
    "max_depth": [-1, 8],
    "learning_rate": [0.03, 0.05],
    "n_estimators": [500, 800],
    "subsample": [0.8, 1.0],
    "colsample_bytree": [0.8, 1.0],
}


def lgb_grid_search(X, y, cv=3, scoring="f1_macro") -> Tuple[Dict[str, Any], float]:
    gs = GridSearchCV(
        _build_lgb(), lgb_param_grid, cv=cv, scoring=scoring, n_jobs=-1, verbose=1
    )
    gs.fit(X, y)
    return gs.best_params_, gs.best_score_


===== C:\Users\barte\OneDrive\Pulpit\Studia\ROK III\Semestr 2\football-prediction\src\pipeline.py =====
import argparse
import json
import logging
from pathlib import Path

import joblib
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import TimeSeriesSplit
from imblearn.over_sampling import SMOTE

from src.constants import FEATURE_COLUMNS
from src.data_loader import load_data
from src.features import extract_features
from src.model import train_model, evaluate_model, get_model
from src.metrics import (
    save_classification_report_txt,
    save_classification_report_json,
    save_confusion_matrix_plot,
)

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger("footpred")


def run_pipeline(
    *,
    algo: str = "lgb",
    input_path: str = "data/processed/model_input.parquet",
    output_dir: str = "output",
) -> None:
    logger.info("Ładowanie danych → %s", input_path)
    df = extract_features(load_data(input_path)).dropna(subset=FEATURE_COLUMNS)
    X, y = df[FEATURE_COLUMNS], df["result"]

    print("Different y classes counts:")
    print(y.value_counts())


    cv = TimeSeriesSplit(n_splits=5)
    train_idx, test_idx = list(cv.split(X))[-1]
    X_tr, X_te, y_tr, y_te = (
        X.iloc[train_idx],
        X.iloc[test_idx],
        y.iloc[train_idx],
        y.iloc[test_idx],
    )

    sm = SMOTE(random_state=42)
    X_resampled, y_resampled = sm.fit_resample(X_tr, y_tr)


    if algo == "stack":
        base = [
            ("lgb", get_model("lgb")),
            ("cat", get_model("cat")),
            ("rf", get_model("rf")),
        ]
        model = StackingClassifier(
            estimators=base,
            final_estimator=get_model("lgb"),
            passthrough=True,
            n_jobs=-1,
        ).fit(X_resampled, y_resampled)
    else:
        model = train_model(X_resampled, y_resampled, algo=algo)


    y_pred = model.predict(X_te)
    metrics = evaluate_model(y_te, y_pred, verbose=False)
    logger.info("Acc %.3f | macro‑F1 %.3f", metrics["accuracy"], metrics["macro_f1"])


    out = Path(output_dir)
    out.mkdir(exist_ok=True, parents=True)
    joblib.dump(model, out / f"final_model_{algo}.pkl")
    (out / "feature_schema.json").write_text(json.dumps(FEATURE_COLUMNS, indent=2))
    save_confusion_matrix_plot(y_te, y_pred, out / "confusion_matrix.png")
    save_classification_report_txt(y_te, y_pred, out / "report.txt")
    save_classification_report_json(y_te, y_pred, out / "metrics.json")
    logger.info("Artefakty zapisane w %s", out.resolve())


if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--algo", choices=["rf", "lgb", "cat", "stack"], default="stack")
    p.add_argument("--input", default="data/processed/model_input.parquet")
    p.add_argument("--output", default="output")
    args = p.parse_args()
    run_pipeline(algo=args.algo, input_path=args.input, output_dir=args.output)

